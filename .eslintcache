[{"/Users/veka/Desktop/Tesis/emotions_ai/src/index.js":"1","/Users/veka/Desktop/Tesis/emotions_ai/src/reportWebVitals.js":"2","/Users/veka/Desktop/Tesis/emotions_ai/src/Routes.js":"3","/Users/veka/Desktop/Tesis/emotions_ai/src/App.js":"4","/Users/veka/Desktop/Tesis/emotions_ai/src/components/blazeface.js":"5","/Users/veka/Desktop/Tesis/emotions_ai/src/components/ManuelModel.js":"6","/Users/veka/Desktop/Tesis/emotions_ai/src/helpers/getEmotion.js":"7"},{"size":554,"mtime":1619913961432,"results":"8","hashOfConfig":"9"},{"size":362,"mtime":1619913867042,"results":"10","hashOfConfig":"9"},{"size":767,"mtime":1619913961432,"results":"11","hashOfConfig":"9"},{"size":4509,"mtime":1619913867041,"results":"12","hashOfConfig":"9"},{"size":6981,"mtime":1619913961432,"results":"13","hashOfConfig":"9"},{"size":15124,"mtime":1626491815488,"results":"14","hashOfConfig":"9"},{"size":412,"mtime":1619930059435,"results":"15","hashOfConfig":"9"},{"filePath":"16","messages":"17","errorCount":0,"warningCount":1,"fixableErrorCount":0,"fixableWarningCount":0,"source":"18","usedDeprecatedRules":"19"},"zzhoou",{"filePath":"20","messages":"21","errorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":"19"},{"filePath":"22","messages":"23","errorCount":0,"warningCount":1,"fixableErrorCount":0,"fixableWarningCount":0,"source":"24","usedDeprecatedRules":"19"},{"filePath":"25","messages":"26","errorCount":0,"warningCount":3,"fixableErrorCount":0,"fixableWarningCount":0,"source":"27","usedDeprecatedRules":"19"},{"filePath":"28","messages":"29","errorCount":0,"warningCount":4,"fixableErrorCount":0,"fixableWarningCount":0,"source":"30","usedDeprecatedRules":"19"},{"filePath":"31","messages":"32","errorCount":0,"warningCount":7,"fixableErrorCount":0,"fixableWarningCount":0,"source":null},{"filePath":"33","messages":"34","errorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":"19"},"/Users/veka/Desktop/Tesis/emotions_ai/src/index.js",["35"],"import React from 'react';\nimport ReactDOM from 'react-dom';\nimport './index.css';\nimport App from './App';\nimport reportWebVitals from './reportWebVitals';\nimport Routes from './Routes';\n\nReactDOM.render(\n  <React.StrictMode>\n    {/* <App /> */}\n    <Routes />\n  </React.StrictMode>,\n  document.getElementById('root')\n);\n\n// If you want to start measuring performance in your app, pass a function\n// to log results (for example: reportWebVitals(console.log))\n// or send to an analytics endpoint. Learn more: https://bit.ly/CRA-vitals\nreportWebVitals();\n",["36","37"],"/Users/veka/Desktop/Tesis/emotions_ai/src/reportWebVitals.js",[],"/Users/veka/Desktop/Tesis/emotions_ai/src/Routes.js",["38"],"import React, { useState } from 'react';\nimport {BrowserRouter, Switch, Route} from 'react-router-dom';\nimport App from './App';\nimport Blazeface from './components/blazeface';\nimport ManuelModel from './components/ManuelModel';\n// import ProtectedRoute from './ProtectedRoute'\n\nfunction Routes() {\n\n  return (\n    <div className=\"Routes\">\n      <BrowserRouter>\n        <Switch>\n          {/* Users */}\n          <Route exact path=\"/\" component={App}/>,\n          <Route exact path=\"/blazeface\" component={Blazeface}/>,\n          <Route exact path=\"/manuel\" component={ManuelModel}/>,\n          {/* <ProtectedRoute exact path=\"/:landing/main\" role={isUser} component={UserMain}/>, */}\n        </Switch>\n      </BrowserRouter>\n    </div>\n  );\n}\n\nexport default Routes;","/Users/veka/Desktop/Tesis/emotions_ai/src/App.js",["39","40","41"],"import React, {useState, useRef, useEffect} from 'react';\nimport * as cocoSsd from \"@tensorflow-models/coco-ssd\";\nimport \"@tensorflow/tfjs\";\nimport logo from './logo.svg';\nimport './App.css';\n\nfunction App() {\n\n  const [sectionClass, setSectionClass] = useState(\"invisible\")\n  let video = document.getElementById('webcam');\n  const liveView = document.getElementById('liveView');\n  // const demosSection = document.getElementById('demos');\n  // const enableWebcamButton = document.getElementById('webcamButton');\n  \n  // Check if webcam access is supported.\n  const getUserMediaSupported= ()=> {\n      return !!(navigator.mediaDevices && navigator.mediaDevices.getUserMedia);\n  }\n    \n  // If webcam supported, add event listener to button for when user\n  // wants to activate it to call enableCam function which we will \n  // define in the next step.\n\n    \n  // Enable the live webcam view and start classification.\n  const enableCam = (event) => {\n\n      // Only continue if the COCO-SSD has finished loading.\n      if (!model) {\n        return;\n      }\n      \n      // Hide the button once clicked.\n      event.target.classList.add('removed');  \n      \n      // getUsermedia parameters to force video but not audio.\n      const constraints = {\n        video: true\n      };\n    \n      // Activate the webcam stream.\n      navigator.mediaDevices.getUserMedia(constraints)\n      .then( (stream) => {\n        video.srcObject = stream;\n        video.addEventListener('loadeddata', predictWebcam);\n      });\n  }\n\n  // Pretend model has loaded so we can try out the webcam code.\n  // Store the resulting model in the global scope of our app.\n  let model = undefined;\n  \n  // Before we can use COCO-SSD class we must wait for it to finish\n  // loading. Machine Learning models can be large and take a moment \n  // to get everything needed to run.\n  // Note: cocoSsd is an external object loaded from our index.html\n  // script tag import so ignore any warning in Glitch.\n  cocoSsd.load()\n  .then( (loadedModel) => {\n    model = loadedModel;\n    // Show demo section now model is ready to use.\n    setSectionClass(\"\")\n  });\n  \n  let children = [];\n  \n  const predictWebcam = () => {\n\n    // Now let's start classifying a frame in the stream.\n    model.detect(video)\n    .then( (predictions) => {\n\n      // Remove any highlighting we did previous frame.\n      for (let i = 0; i < children.length; i++) {\n        liveView.removeChild(children[i]);\n      }\n      children.splice(0);\n      \n      // Now lets loop through predictions and draw them to the live view if\n      // they have a high confidence score.\n      for (let n = 0; n < predictions.length; n++) {\n\n        // If we are over 66% sure we are sure we classified it right, draw it!\n        if (predictions[n].score > 0.66) {\n          \n          const p = document.createElement('p');\n          p.innerText = predictions[n].class  + ' - with ' \n              + Math.round(parseFloat(predictions[n].score) * 100) \n              + '% confidence.';\n          p.style = 'margin-left: ' + predictions[n].bbox[0] + 'px; margin-top: '\n              + (predictions[n].bbox[1] - 10) + 'px; width: ' \n              + (predictions[n].bbox[2] - 10) + 'px; top: 0; left: 0;';\n  \n          const highlighter = document.createElement('div');\n          highlighter.setAttribute('class', 'highlighter');\n          highlighter.style = 'left: ' + predictions[n].bbox[0] + 'px; top: '\n              + predictions[n].bbox[1] + 'px; width: ' \n              + predictions[n].bbox[2] + 'px; height: '\n              + predictions[n].bbox[3] + 'px;';\n  \n          liveView.appendChild(highlighter);\n          liveView.appendChild(p);\n          children.push(highlighter);\n          children.push(p);\n        }\n      }\n      \n      // Call this function again to keep predicting when the browser is ready.\n      window.requestAnimationFrame(predictWebcam);\n\n    });\n  }\n  \n\n  const handleEnableCamera = (event) => {\n\n    if (getUserMediaSupported()) {\n      enableCam(event);\n    } else {\n      console.warn('getUserMedia() is not supported by your browser');\n    }\n\n  }\n  \n\n\n  return (\n    <div className=\"App\">\n\n      <h1>Emotions detection</h1>\n\n\n    \n    <section id=\"demos\" className={sectionClass}>\n\n   \n      \n      <div id=\"liveView\" className=\"camView\">\n        <button id=\"webcamButton\" onClick={handleEnableCamera}>Enable Webcam</button>\n        <video id=\"webcam\" autoPlay width=\"640\" height=\"480\"></video>\n      </div>\n    </section>\n</div>\n  );\n}\n\nexport default App;\n","/Users/veka/Desktop/Tesis/emotions_ai/src/components/blazeface.js",["42","43","44","45"],"import React, { useEffect, useState } from 'react'\nimport PropTypes from 'prop-types'\nimport * as tf from '@tensorflow/tfjs-core';\nimport * as tfjsWasm from '@tensorflow/tfjs-backend-wasm';\n// tfjsWasm.setWasmPath('https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-wasm@latest/dist/tfjs-backend-wasm.wasm');\n\nconst Blazeface = props => {\n    \n    const blazeface = require('@tensorflow-models/blazeface');\n\n    const [sectionClass, setSectionClass] = useState(\"invisible\");\n    let video = document.getElementById('webcam');\n    const liveView = document.getElementById('liveView');\n    // Pretend model has loaded so we can try out the webcam code.\n    // Store the resulting model in the global scope of our app.\n    const [model, setModel] = useState(undefined);\n\n    let ctx, videoWidth, videoHeight, canvas;\n\n    const state = {\n        backend: 'wasm'\n    };\n\n\n    // Check if webcam access is supported.\n    const getUserMediaSupported= ()=> {\n        return !!(navigator.mediaDevices && navigator.mediaDevices.getUserMedia);\n    }\n\n    const predictWebcam = () => {\n\n        video.play()\n        videoWidth = video.videoWidth;\n        videoHeight = video.videoHeight;\n        video.width = videoWidth;\n        video.height = videoHeight;\n      \n        canvas = document.getElementById('output');\n        canvas.width = videoWidth;\n        canvas.height = videoHeight;\n        ctx = canvas.getContext('2d');\n        ctx.fillStyle = \"rgba(255, 0, 0, 0.5)\";\n        \n\n        const returnTensors = false; // Pass in `true` to get tensors back, rather than values.\n        const flipHorizontal = true;\n        const annotateBoxes = true;\n\n        console.log('listo para hacer AI');\n\n        model.estimateFaces(video, returnTensors, \n            flipHorizontal, annotateBoxes\n        )\n        .then( predictions => {\n\n            console.log('good news');\n\n            if (predictions.length > 0) {\n                ctx.clearRect(0, 0, canvas.width, canvas.height);\n                /*\n                `predictions` is an array of objects describing each detected face, for example:\n            \n                [\n                    {\n                    topLeft: [232.28, 145.26],\n                    bottomRight: [449.75, 308.36],\n                    probability: [0.998],\n                    landmarks: [\n                        [295.13, 177.64], // right eye\n                        [382.32, 175.56], // left eye\n                        [341.18, 205.03], // nose\n                        [345.12, 250.61], // mouth\n                        [252.76, 211.37], // right ear\n                        [431.20, 204.93] // left ear\n                    ]\n                    }\n                ]\n                */\n            \n                for (let i = 0; i < predictions.length; i++) {\n\n                    if (returnTensors) {\n\n                        predictions[i].topLeft = predictions[i].topLeft.arraySync();\n                        predictions[i].bottomRight = predictions[i].bottomRight.arraySync();\n                        if (annotateBoxes) {\n                            predictions[i].landmarks = predictions[i].landmarks.arraySync();\n                        }\n                    }\n                    const start = predictions[i].topLeft;\n                    const end = predictions[i].bottomRight;\n                    const size = [end[0] - start[0], end[1] - start[1]];\n            \n                    // Render a rectangle over each detected face.\n                    ctx.fillStyle = \"rgba(255, 0, 0, 0.5)\";\n                    ctx.fillRect(start[0], start[1], size[0], size[1]);\n\n                    if (annotateBoxes) {\n                        const landmarks = predictions[i].landmarks;\n                \n                        ctx.fillStyle = \"blue\";\n                        for (let j = 0; j < landmarks.length; j++) {\n                          const x = landmarks[j][0];\n                          const y = landmarks[j][1];\n                          ctx.fillRect(x, y, 5, 5);\n                        }\n                    }\n                }\n            }\n\n            requestAnimationFrame(predictWebcam)\n        })\n    }\n\n    // Enable the live webcam view and start classification.\n    const enableCam = (event) => {\n\n        // Only continue if the COCO-SSD has finished loading.\n        if (!model) {\n            console.log('no hay modelo');\n            return;\n        } else {\n            console.log('habemus modelo');\n            // Hide the button once clicked.\n            event.target.classList.add('removed'); \n\n            // getUsermedia parameters to force video but not audio.\n            const constraints = {\n                // video: true\n                video: { facingMode: 'user' }\n            };\n\n            // Activate the webcam stream.\n            navigator.mediaDevices.getUserMedia(constraints)\n            .then( (stream) => {\n                console.log('camara habilitada');\n                video.srcObject = stream;\n                video.addEventListener('loadeddata', predictWebcam);\n            })\n            .catch( error => {\n                console.log('error para habilitar la camara');\n            })\n        }\n    }\n\n\n    const handleEnableCamera = (event) => {\n\n        if (getUserMediaSupported()) {\n            enableCam(event);\n        } else {\n            console.warn('getUserMedia() is not supported by your browser');\n        }\n\n    }\n\n\n    const setupPage = () => {\n  \n        tf.setBackend(state.backend).then(() => {\n            console.log('back ready ');\n            blazeface.load().then( (loadedModel) => {\n\n                setModel(loadedModel)\n                setSectionClass(\"\")\n                console.log('modelo cargado');\n            })\n            .catch( error => {\n                console.log(error);\n                console.log('no cargo');\n            })\n        })\n\n      };\n\n    useEffect(() => {\n        setupPage()\n        return () => {\n        }\n    }, [])\n\n\n    return (\n        <div>\n            <div>BLAZEFACE</div>\n            <section id=\"demos\" className={sectionClass}>\n                <button \n                    id=\"webcamButton\" \n                    onClick={handleEnableCamera} \n                >\n                    Enable Webcam\n                </button>\n                <div id=\"liveView\" className=\"camView\">   \n                    <video \n                        id=\"webcam\" \n                        autoPlay \n                        width=\"640\" \n                        height=\"480\"\n                        // playsinline \n                        style={{\n                            \"-webkit-transform\":\"scaleX(-1)\",\n                            \"transform\": \"scaleX(-1)\"\n                        }}\n                    >\n                    </video>\n                    <canvas id=\"output\"></canvas>\n                    \n                </div>\n            </section>\n            \n        </div>\n    )\n}\n\nBlazeface.propTypes = {\n\n}\n\nexport default Blazeface\n","/Users/veka/Desktop/Tesis/emotions_ai/src/components/ManuelModel.js",["46","47","48","49","50","51","52"],"/Users/veka/Desktop/Tesis/emotions_ai/src/helpers/getEmotion.js",[],{"ruleId":"53","severity":1,"message":"54","line":4,"column":8,"nodeType":"55","messageId":"56","endLine":4,"endColumn":11},{"ruleId":"57","replacedBy":"58"},{"ruleId":"59","replacedBy":"60"},{"ruleId":"53","severity":1,"message":"61","line":1,"column":17,"nodeType":"55","messageId":"56","endLine":1,"endColumn":25},{"ruleId":"53","severity":1,"message":"62","line":1,"column":26,"nodeType":"55","messageId":"56","endLine":1,"endColumn":32},{"ruleId":"53","severity":1,"message":"63","line":1,"column":34,"nodeType":"55","messageId":"56","endLine":1,"endColumn":43},{"ruleId":"53","severity":1,"message":"64","line":4,"column":8,"nodeType":"55","messageId":"56","endLine":4,"endColumn":12},{"ruleId":"53","severity":1,"message":"65","line":2,"column":8,"nodeType":"55","messageId":"56","endLine":2,"endColumn":17},{"ruleId":"53","severity":1,"message":"66","line":4,"column":13,"nodeType":"55","messageId":"56","endLine":4,"endColumn":21},{"ruleId":"53","severity":1,"message":"67","line":13,"column":11,"nodeType":"55","messageId":"56","endLine":13,"endColumn":19},{"ruleId":"68","severity":1,"message":"69","line":180,"column":8,"nodeType":"70","endLine":180,"endColumn":10,"suggestions":"71"},{"ruleId":"53","severity":1,"message":"65","line":2,"column":8,"nodeType":"55","messageId":"56","endLine":2,"endColumn":17},{"ruleId":"53","severity":1,"message":"66","line":5,"column":13,"nodeType":"55","messageId":"56","endLine":5,"endColumn":21},{"ruleId":"53","severity":1,"message":"72","line":66,"column":11,"nodeType":"55","messageId":"56","endLine":66,"endColumn":15},{"ruleId":"53","severity":1,"message":"73","line":67,"column":9,"nodeType":"55","messageId":"56","endLine":67,"endColumn":12},{"ruleId":"74","severity":1,"message":"75","line":280,"column":51,"nodeType":"76","messageId":"77","endLine":280,"endColumn":53},{"ruleId":"74","severity":1,"message":"75","line":303,"column":37,"nodeType":"76","messageId":"77","endLine":303,"endColumn":39},{"ruleId":"68","severity":1,"message":"69","line":421,"column":6,"nodeType":"70","endLine":421,"endColumn":8,"suggestions":"78"},"no-unused-vars","'App' is defined but never used.","Identifier","unusedVar","no-native-reassign",["79"],"no-negated-in-lhs",["80"],"'useState' is defined but never used.","'useRef' is defined but never used.","'useEffect' is defined but never used.","'logo' is defined but never used.","'PropTypes' is defined but never used.","'tfjsWasm' is defined but never used.","'liveView' is assigned a value but never used.","react-hooks/exhaustive-deps","React Hook useEffect has a missing dependency: 'setupPage'. Either include it or remove the dependency array.","ArrayExpression",["81"],"'cont' is assigned a value but never used.","'aux' is assigned a value but never used.","array-callback-return","Array.prototype.map() expects a return value from arrow function.","ArrowFunctionExpression","expectedInside",["82"],"no-global-assign","no-unsafe-negation",{"desc":"83","fix":"84"},{"desc":"83","fix":"85"},"Update the dependencies array to be: [setupPage]",{"range":"86","text":"87"},{"range":"88","text":"87"},[5959,5961],"[setupPage]",[13555,13557]]